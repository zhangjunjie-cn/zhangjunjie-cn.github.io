# scrapy爬虫详解

## 爬虫的原理

**废话**：这个月前几天才开学，事比较少，所以就自学了一下scrapy来爬取B站文章以及标题，后面感觉数据不够，就又爬了每篇文章的点赞数以及评论观看投币数等等来做NLP中的训练；

我们先简单讲一下爬虫的原理，但不细讲，知道就行，因为后面是直接用框架，然后你遇到问题再去查印象会更深；
### 整个程序运行的过程

 1. 表明自己的身份，我是谁（headers），我在哪（ip地址），我要干什么，当然是爬数据了~
 2. 有了这些信息，下一步当然就是干最重要的事了，请求数据（request）从服务器中获取数据并返回（response），你问服务器为什么要给你数据，因为你是用户啊，服务器就是给用户服务的，我们之前是伪装了用户的身份信息的，但如果你是python的身份信息，那它当然不会给你数据（解决反爬机制）
 3. 返回的信息是一大堆数据，这些东西可以通过浏览器的渲染排好版地展示再用户地面前，但是我们当然不可能每次都通过浏览器来人工地处理数据，所以需要使用一种规则来获取我们想要的数据（这里地数据每次地格式是差不多的，如果有差别需要重新设计规则）比如：正则表达式、xpath、css选择器等等，当然，这些都是针对静态数据，动态数据需要我们去浏览器手动分析，这里大概了解就行，后面会继续讲解；
 4. 最后就是忙活了这么久，收获的时候了（保存数据），选择合理的保存方法对后面处理这些数据是很关键的，但其实影响也不会太大，大多数也都可以后面相互转换，常见的方法就是txt、csv、json等格式，或者直接保存到数据库里面；

**注：** *这里涉及到很多专有词，如果你才接触爬虫的话，对于这些是很陌生的，请不要被吓着，这些词汇百度一下一两句话你就懂了，而且后面也会在例子中尽量讲到*

### 介绍scrapy的运行过程
*如果你在上面对爬虫爬取的过程还有点模糊的话，那么这里就能加深你的理解了，毕竟scrapy框架也是爬虫，原理是一致的，只是功能多了不少，在这么多功能的前提下，我们仅仅只需要遵守它的编程规则就可以了*
**话不多说，看图：**
![image-20221021153529204](https://oss.justin3go.com/blogs/image-20221021153529204.png)

#### 先介绍它的5+2结构的组件

 1. ENGINE：这个可以说是最重要但也最不重要的组件了，一方面是因为所有其他的组件都是通过这个引擎来进行活动的，另一方面是这个我们是不用对其进行任何操作的，真好~
 2. SCHEDULER：爬虫的时间调度器，因为scrapy是自带多线程异步爬取的，你问我多线程是什么，一个字“快”
 3. DOWNLOADER：下载器，去服务器获取数据，相当于整个框架的前线士兵；
 4. ITEM PIPELINES：前面不是说了最后一步是保存数据码，这个地方就是scrapy框架给我们来保存数据的地方；
 5. SPIDER：设置爬虫的地方
 6. MIDDLEWARE：中间件（自带有两个中间件，也可以自己创建中间件，需要遵循规则，这篇文章不会细讲）本来上面五个结构就能满足我们爬取数据的需求，但是由于一些反爬机制或者其他的原因，比如url本来就是没数据的等等，而导致我们的程序被终止，这显然是不具有鲁棒性的，不够灵活，所以scrapy就增加了这两个中间键让用户自定义处理一些异常等
 7. 6包含两个组件，从图中我们可以看出这两个组件只是处理的地方不同而已，一个是在形成爬虫那里，一个是在下载数据那里
 ### 过程
 其实图中已经很详细了，不过咋们是保姆级教程，所以再来解释一番：
 1-4：爬虫通过引擎调动时间表去网页下载内容，里面是通过requests完成的；
 5-6：返回内容并交给爬虫里面的parse方法进行解析获取自己想要的内容；
 7-8：保存内容，item是你要保存内容的对象（就是学生：姓名、学号这些）
 pipline就是保存你设计的item对象为某种格式；

 **挺简单的，后面看了代码你的整体逻辑会更加清晰**
 ## 实战开始~
 如果你真的是小白小白，安装anaconda3 -> 创建环境（conda create -n <环境名> python=3.7）-> pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple 
 好吧，现在开始实操->

 **激活你的环境**（我的爬虫环境名为EasyTitle，这里我是在VSCODE里面使用的终端，你也可以在任何编译器里面这样做，anaconda自带很多编译器，也可以使用anaconda_promapt，同时你也要选择之后项目所在的位置，就是路径: cd <路径名>）：
![image-20221021153603035](https://oss.justin3go.com/blogs/image-20221021153603035.png))
**创建项目** 在终端输入scrapy startproject bili , 其中bili是我的项目名
![image-20221021153619681](https://oss.justin3go.com/blogs/image-20221021153619681.png)
然后，你的该目录下就会出现一个bili的文件夹，这就是scrapy工作的地方，cd bili 进入这个文件夹（这步就不展示图了）；该文件夹如下
![image-20221021153634207](https://oss.justin3go.com/blogs/image-20221021153634207.png)
接下来就是重点了，打好精神~
bili项目下分两个文件< bili >< scrapy.cfg >,bili里面有spider、items、middlewares、piplines、setting这些文件或文件夹（不提init)

1. spider文件夹里面是装着你的爬虫，后面生成的爬虫都会存放在该目录
2. items容器设计你的保存对象
3. pipline保存内容到文件
4. miiddleware自定义处理可能会出现的一些异常情况
5. settings一些设置，比如启动关闭middleware，设置每次爬取的延时时间等等
    基于上面的理解，我们我们来一步步分析：
    首先生成我们的一只爬虫：

---

（后续更新）算了，懒得写了，直接看源码吧：https://github.com/Justin3go/Bili_Spider-bili_dataset-

