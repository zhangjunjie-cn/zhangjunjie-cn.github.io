import{_ as n}from"./chunks/WDocTitleMeta.C1-iBLm4.js";import{_ as p,C as c,c as d,o as r,j as s,G as g,aU as m,a as h,w as u,b as _,e as y}from"./chunks/framework.C_70-4oE.js";import"./chunks/theme.Dn7iT2bj.js";const S=JSON.parse('{"title":"scrapy爬虫详解","description":"","frontmatter":{},"headers":[],"relativePath":"博客/2021/01scrapy爬虫详解.md","filePath":"博客/2021/01scrapy爬虫详解.md","lastUpdated":1760196138000}'),b={name:"博客/2021/01scrapy爬虫详解.md"};function f(t,a,E,P,q,k){const l=n,o=c("ClientOnly");return r(),d("div",null,[a[0]||(a[0]=s("h1",{id:"scrapy爬虫详解",tabindex:"-1"},[h("scrapy爬虫详解 "),s("a",{class:"header-anchor",href:"#scrapy爬虫详解","aria-label":'Permalink to "scrapy爬虫详解"'},"​")],-1)),g(o,null,{default:u(()=>{var e,i;return[(((e=t.$frontmatter)==null?void 0:e.aside)??!0)&&(((i=t.$frontmatter)==null?void 0:i.showWDocTitleMeta)??!0)?(r(),_(l,{key:0,article:t.$frontmatter},null,8,["article"])):y("",!0)]}),_:1}),a[1]||(a[1]=m('<h2 id="爬虫的原理" tabindex="-1">爬虫的原理 <a class="header-anchor" href="#爬虫的原理" aria-label="Permalink to &quot;爬虫的原理&quot;">​</a></h2><p><strong>废话</strong>：这个月前几天才开学，事比较少，所以就自学了一下scrapy来爬取B站文章以及标题，后面感觉数据不够，就又爬了每篇文章的点赞数以及评论观看投币数等等来做NLP中的训练；</p><p>我们先简单讲一下爬虫的原理，但不细讲，知道就行，因为后面是直接用框架，然后你遇到问题再去查印象会更深；</p><h3 id="整个程序运行的过程" tabindex="-1">整个程序运行的过程 <a class="header-anchor" href="#整个程序运行的过程" aria-label="Permalink to &quot;整个程序运行的过程&quot;">​</a></h3><ol><li>表明自己的身份，我是谁（headers），我在哪（ip地址），我要干什么，当然是爬数据了~</li><li>有了这些信息，下一步当然就是干最重要的事了，请求数据（request）从服务器中获取数据并返回（response），你问服务器为什么要给你数据，因为你是用户啊，服务器就是给用户服务的，我们之前是伪装了用户的身份信息的，但如果你是python的身份信息，那它当然不会给你数据（解决反爬机制）</li><li>返回的信息是一大堆数据，这些东西可以通过浏览器的渲染排好版地展示再用户地面前，但是我们当然不可能每次都通过浏览器来人工地处理数据，所以需要使用一种规则来获取我们想要的数据（这里地数据每次地格式是差不多的，如果有差别需要重新设计规则）比如：正则表达式、xpath、css选择器等等，当然，这些都是针对静态数据，动态数据需要我们去浏览器手动分析，这里大概了解就行，后面会继续讲解；</li><li>最后就是忙活了这么久，收获的时候了（保存数据），选择合理的保存方法对后面处理这些数据是很关键的，但其实影响也不会太大，大多数也都可以后面相互转换，常见的方法就是txt、csv、json等格式，或者直接保存到数据库里面；</li></ol><p><strong>注：</strong> <em>这里涉及到很多专有词，如果你才接触爬虫的话，对于这些是很陌生的，请不要被吓着，这些词汇百度一下一两句话你就懂了，而且后面也会在例子中尽量讲到</em></p><h3 id="介绍scrapy的运行过程" tabindex="-1">介绍scrapy的运行过程 <a class="header-anchor" href="#介绍scrapy的运行过程" aria-label="Permalink to &quot;介绍scrapy的运行过程&quot;">​</a></h3><p><em>如果你在上面对爬虫爬取的过程还有点模糊的话，那么这里就能加深你的理解了，毕竟scrapy框架也是爬虫，原理是一致的，只是功能多了不少，在这么多功能的前提下，我们仅仅只需要遵守它的编程规则就可以了</em><strong>话不多说，看图：</strong><img src="https://oss.justin3go.com/blogs/image-20221021153529204.png" alt="image-20221021153529204" loading="lazy"></p><h4 id="先介绍它的5-2结构的组件" tabindex="-1">先介绍它的5+2结构的组件 <a class="header-anchor" href="#先介绍它的5-2结构的组件" aria-label="Permalink to &quot;先介绍它的5+2结构的组件&quot;">​</a></h4><ol><li>ENGINE：这个可以说是最重要但也最不重要的组件了，一方面是因为所有其他的组件都是通过这个引擎来进行活动的，另一方面是这个我们是不用对其进行任何操作的，真好~</li><li>SCHEDULER：爬虫的时间调度器，因为scrapy是自带多线程异步爬取的，你问我多线程是什么，一个字“快”</li><li>DOWNLOADER：下载器，去服务器获取数据，相当于整个框架的前线士兵；</li><li>ITEM PIPELINES：前面不是说了最后一步是保存数据码，这个地方就是scrapy框架给我们来保存数据的地方；</li><li>SPIDER：设置爬虫的地方</li><li>MIDDLEWARE：中间件（自带有两个中间件，也可以自己创建中间件，需要遵循规则，这篇文章不会细讲）本来上面五个结构就能满足我们爬取数据的需求，但是由于一些反爬机制或者其他的原因，比如url本来就是没数据的等等，而导致我们的程序被终止，这显然是不具有鲁棒性的，不够灵活，所以scrapy就增加了这两个中间键让用户自定义处理一些异常等</li><li>6包含两个组件，从图中我们可以看出这两个组件只是处理的地方不同而已，一个是在形成爬虫那里，一个是在下载数据那里</li></ol><h3 id="过程" tabindex="-1">过程 <a class="header-anchor" href="#过程" aria-label="Permalink to &quot;过程&quot;">​</a></h3><p>其实图中已经很详细了，不过咋们是保姆级教程，所以再来解释一番： 1-4：爬虫通过引擎调动时间表去网页下载内容，里面是通过requests完成的； 5-6：返回内容并交给爬虫里面的parse方法进行解析获取自己想要的内容； 7-8：保存内容，item是你要保存内容的对象（就是学生：姓名、学号这些） pipline就是保存你设计的item对象为某种格式；</p><p><strong>挺简单的，后面看了代码你的整体逻辑会更加清晰</strong></p><h2 id="实战开始" tabindex="-1">实战开始~ <a class="header-anchor" href="#实战开始" aria-label="Permalink to &quot;实战开始~&quot;">​</a></h2><p>如果你真的是小白小白，安装anaconda3 -&gt; 创建环境（conda create -n &lt;环境名&gt; python=3.7）-&gt; pip install scrapy -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noreferrer">https://pypi.tuna.tsinghua.edu.cn/simple</a> 好吧，现在开始实操-&gt;</p><p><strong>激活你的环境</strong>（我的爬虫环境名为EasyTitle，这里我是在VSCODE里面使用的终端，你也可以在任何编译器里面这样做，anaconda自带很多编译器，也可以使用anaconda_promapt，同时你也要选择之后项目所在的位置，就是路径: cd &lt;路径名&gt;）： <img src="https://oss.justin3go.com/blogs/image-20221021153603035.png" alt="image-20221021153603035" loading="lazy">) <strong>创建项目</strong> 在终端输入scrapy startproject bili , 其中bili是我的项目名 <img src="https://oss.justin3go.com/blogs/image-20221021153619681.png" alt="image-20221021153619681" loading="lazy"> 然后，你的该目录下就会出现一个bili的文件夹，这就是scrapy工作的地方，cd bili 进入这个文件夹（这步就不展示图了）；该文件夹如下 <img src="https://oss.justin3go.com/blogs/image-20221021153634207.png" alt="image-20221021153634207" loading="lazy"> 接下来就是重点了，打好精神~ bili项目下分两个文件&lt; bili &gt;&lt; scrapy.cfg &gt;,bili里面有spider、items、middlewares、piplines、setting这些文件或文件夹（不提init)</p><ol><li>spider文件夹里面是装着你的爬虫，后面生成的爬虫都会存放在该目录</li><li>items容器设计你的保存对象</li><li>pipline保存内容到文件</li><li>miiddleware自定义处理可能会出现的一些异常情况</li><li>settings一些设置，比如启动关闭middleware，设置每次爬取的延时时间等等 基于上面的理解，我们我们来一步步分析： 首先生成我们的一只爬虫：</li></ol><hr><p>（后续更新）算了，懒得写了，直接看源码吧：<a href="https://github.com/Justin3go/Bili_Spider-bili_dataset-" target="_blank" rel="noreferrer">https://github.com/Justin3go/Bili_Spider-bili_dataset-</a></p>',19))])}const T=p(b,[["render",f]]);export{S as __pageData,T as default};
